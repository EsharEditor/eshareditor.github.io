<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>EsharEditor</title>
  <subtitle>Do whatever you want.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://eshareditor.github.io/"/>
  <updated>2016-09-15T09:34:43.111Z</updated>
  <id>https://eshareditor.github.io/</id>
  
  <author>
    <name>Kyle Joe</name>
    <email>eshareditor@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hbase BulkLoad</title>
    <link href="https://eshareditor.github.io/2016/09/14/Hbase-BulkLoad/"/>
    <id>https://eshareditor.github.io/2016/09/14/Hbase-BulkLoad/</id>
    <published>2016-09-14T08:40:23.000Z</published>
    <updated>2016-09-15T09:34:43.111Z</updated>
    
    <content type="html"><![CDATA[<p>Hbase快速数据导入工具BulkLoad：利用Hbase的数据以特定的格式存储在HDFS内这一原理，通过MapReduce作业直接生成这种数据格式文件，然后上传到HDFS上合适的位置，完成大量数据快速入库。</p>
<img src="/uploads/Hbase-BulkLoad/Hbase-BulkLoad.jpg">
<a id="more"></a>
<h2 id="Hbase数据带入方式"><a href="#Hbase数据带入方式" class="headerlink" title="Hbase数据带入方式"></a>Hbase数据带入方式</h2><ul>
<li>HBase Client 调用方式</li>
<li>MapReduce 任务方式</li>
<li>BulkLoad 工具方式</li>
<li>Sqoop 工具方式</li>
</ul>
<h2 id="BulkLoad优劣势"><a href="#BulkLoad优劣势" class="headerlink" title="BulkLoad优劣势"></a>BulkLoad优劣势</h2><h3 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h3><ul>
<li>如果一次性入导入Hbase表的数据量巨大，不仅处理速度慢不说，还特别占用Region资源， 一个比较高效便捷的方法就是使用 “Bulk Loading”方法，即HBase提供的HFileOutputFormat类。</li>
<li>利用Hbase的数据以特定的格式存储在HDFS内这一原理，直接生成这种HDFS中存储的数据格式文件，然后上传至合适位置，即完成巨量数据快速入库的办法。配合mapreduce完成，高效便捷，而且不占用region资源，增添负载。</li>
</ul>
<h3 id="劣势"><a href="#劣势" class="headerlink" title="劣势"></a>劣势</h3><ul>
<li>仅适合初次数据导入，即表内数据为空。</li>
<li>HBase集群与Hadoop集群为同一集群，即HBase所基于的HDFS为生成HFile的MR的集群。</li>
</ul>
<h2 id="BulkLoad使用"><a href="#BulkLoad使用" class="headerlink" title="BulkLoad使用"></a>BulkLoad使用</h2><ol>
<li>从数据源（通常为文本文件或其它数据库）提取数据并上传到HDFS。</li>
<li>利用一个MapReduce作业准备数据：通过编排一个MapReduce作业，需手动编写map函数，作业需要使用rowkey(行键)作为输出Key，KeyValue、Put或者Delete作为输出Value。MapReduce作业需要使用Hbase提供的HFileOutputFormat2来生成Hbase底层存储的HFile数据格式文件。为了有效的导入数据，需要配置HFileOutputFormat2使得每一个输出文件都在一个合适的区域中。</li>
<li>告诉RegionServers数据的位置并导入数据，这一步需要使用Hbase中的LoadIncrementalHFiles，将文件在HDFS上的位置传递给它，它就会利用RegionServer将数据导入到相应的区域，完成与Hbase表的关联。<img src="/uploads/Hbase-BulkLoad/Hbase-BulkLoad.jpg" title="操作流程图">
</li>
</ol>
<h3 id="命令行导入"><a href="#命令行导入" class="headerlink" title="命令行导入"></a>命令行导入</h3><h4 id="直接导入数据"><a href="#直接导入数据" class="headerlink" title="直接导入数据"></a>直接导入数据</h4><p>在Hbase中创建空表，如userinfo,使用Hbase自带的ImportTsv工具导入/opt/userinfo.tsv数据文件</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hadoop jar $HBASE_HOME<span class="regexp">/lib/</span>hbase-server<span class="number">-1.1</span><span class="number">.2</span>.jar importtsv -Dimporttsv.columns=HBASE_ROW_KEY,<span class="string">cf1:</span>name,<span class="string">cf1:</span>sex,<span class="string">cf2:</span><span class="class"><span class="keyword">class</span> <span class="title">userinfo</span> /<span class="title">opt</span>/<span class="title">userinfo</span>.<span class="title">tsv</span></span></div></pre></td></tr></table></figure>
<h4 id="分步导入数据"><a href="#分步导入数据" class="headerlink" title="分步导入数据"></a>分步导入数据</h4><ul>
<li>将userinfo.tsv文件上传到HDFS某个目录，并在Hbase中创建userinfo表</li>
</ul>
<figure class="highlight awk"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hadoop fs -put <span class="regexp">/opt/u</span>serinfo.tsv <span class="regexp">/data_dir/</span>source_file</div></pre></td></tr></table></figure>
<ul>
<li>利用ImportTsv工具的Dimporttsv.bulk.output参数项指定生成的HFile文件位置/date_dir/bulkload_data/output</li>
</ul>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hadoop jar $HBASE_HOME/lib/hbase-server-<span class="number">1.1</span>.<span class="number">2</span>.jar importtsv -Dimporttsv.bulk.output=<span class="regexp">/date_dir/bulkload</span>_data/output -Dimporttsv.columns=HBASE_ROW_KEY,<span class="symbol">cf1:</span>name,<span class="symbol">cf1:</span>sex,<span class="symbol">cf2:</span><span class="class"><span class="keyword">class</span> <span class="title">userinfo</span> /<span class="title">opt</span>/<span class="title">userinfo</span>.<span class="title">tsv</span></span></div></pre></td></tr></table></figure>
<ul>
<li>利用completebulkload完成bluk load数据导入</li>
</ul>
<figure class="highlight crystal"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hadoop jar $HBASE_HOME/<span class="class"><span class="keyword">lib</span>/<span class="title">hbase</span>-<span class="title">server</span>-1.1.2.<span class="title">jar</span> <span class="title">completebulkload</span> /<span class="title">date_dir</span>/<span class="title">bulkload_data</span>/<span class="title">output</span> <span class="title">userinfo</span></span></div></pre></td></tr></table></figure>
<h3 id="相关说明"><a href="#相关说明" class="headerlink" title="相关说明"></a>相关说明</h3><ol>
<li>将数据文件生成HFile方式是所有的加载方案里面是最快的，前提是：Hbase表须为空！如果表中已经有数据，HFile再次导入的时候，HBase的表会触发split分割操作</li>
<li>最终输出结果，无论是Map还是Reduce，输出建议只使用<immutablebyteswritable, keyvalue=""></immutablebyteswritable,></li>
<li>指定的列和源文件字段必须全部匹配，不匹配则一定导入失败</li>
<li>命令行导入时默认的分隔符为tab符</li>
<li>RowKey可以在任何位置：<br>假设存在数据4列（逗号为分隔符）<br>1，2，3，4<br>第一列RowKey<br>RowKey，2，3，4<br>第三列RowKey<br>1，2，RowKey，4</li>
<li>常用参数说明：<br>-Dimporttsv.skip.bad.lines=false –&gt; 若遇到无效行则失败<br>‘-Dimporttsv.separator=|’或‘-Dimporttsv.separator=,’ –&gt; 指定分割符<br>-Dimporttsv.timestamp=currentTimeAsLong –&gt; 导入时使用指定的时间戳<br>-Dimporttsv.mapper.class=my.Mapper –&gt; 使用用户指定的Mapper类(默认的org.apache.hadoop.hbase.mapreduce.TsvImporterMapper)</li>
</ol>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p><a href="http://www.ibm.com/developerworks/cn/opensource/os-cn-data-import/index.html" target="_blank" rel="external">http://www.ibm.com/developerworks/cn/opensource/os-cn-data-import/index.html</a><br><a href="http://www.aboutyun.com/thread-11652-1-1.html" target="_blank" rel="external">http://www.aboutyun.com/thread-11652-1-1.html</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hbase快速数据导入工具BulkLoad：利用Hbase的数据以特定的格式存储在HDFS内这一原理，通过MapReduce作业直接生成这种数据格式文件，然后上传到HDFS上合适的位置，完成大量数据快速入库。&lt;/p&gt;
&lt;img src=&quot;/uploads/Hbase-BulkLoad/Hbase-BulkLoad.jpg&quot;&gt;
    
    </summary>
    
      <category term="BigData" scheme="https://eshareditor.github.io/categories/BigData/"/>
    
    
      <category term="Hbase" scheme="https://eshareditor.github.io/tags/Hbase/"/>
    
  </entry>
  
  <entry>
    <title>Hue 编译安装</title>
    <link href="https://eshareditor.github.io/2016/09/14/Hue-%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85/"/>
    <id>https://eshareditor.github.io/2016/09/14/Hue-编译安装/</id>
    <published>2016-09-14T01:46:21.000Z</published>
    <updated>2016-09-14T06:41:47.180Z</updated>
    
    <content type="html"><![CDATA[<p>Hue是一个能够与Apache Hadoop交互的Web应用程序，一个开源的Apache Hadoop UI。<br>本文介绍Hue的编译、安装、启动过程以及记录这个过程中可能遇到的问题。</p>
<img src="/uploads/Hue-编译安装/hue-search.png">
<a id="more"></a>
<h2 id="编译、安装、启动"><a href="#编译、安装、启动" class="headerlink" title="编译、安装、启动"></a>编译、安装、启动</h2><h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><ul>
<li>IOS: Centos6.7  (minimum)</li>
<li>Maven 3.3.3</li>
<li>Ant 1.9.4+ </li>
<li>Python 2.6.6+</li>
</ul>
<h3 id="依赖的工具包"><a href="#依赖的工具包" class="headerlink" title="依赖的工具包"></a>依赖的工具包</h3><ul>
<li><p>Hue官网列出的工具包</p>
<blockquote>
<p>asciidoc krb5-devel libxml2-devel libxslt-devel libtidy mysql mysql-devel openldap-devel python-devel sqlite-devel make openssl-devel gcc gcc-c++ gmp-devel cyrus-sasl-plain cyrus-sasl-devel cyrus-sasl-gssapi libffi-devel</p>
</blockquote>
</li>
<li><p>额外需要的工具包(在编译过程中根据报错信息需额外安装的工具包)</p>
<blockquote>
<p>python-simplejson python-setuptools rsync saslwrapper-devel pycrypto libyaml-devel libsasl2-dev libsasl2-modules-gssapi-mit libkrb5-dev libssl-devel</p>
</blockquote>
</li>
<li><p>注意：Centos7系统安装：gmp libtidy mysql-devel工具无法安装。且在编译过程中提示需安装ipdb: centos7系统自带的python2.7，而默认情况下python2.7不提供pip工具的，需手动安装好，或者使用hue里的python2.7的pip工具来安装ipdb。命令：pip install ipdb</p>
</li>
</ul>
<h3 id="编译安装"><a href="#编译安装" class="headerlink" title="编译安装"></a>编译安装</h3><ul>
<li>下载</li>
</ul>
<figure class="highlight crmsh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ git <span class="keyword">clone</span> <span class="title">https</span>://github.com/cloudera/hue.git</div></pre></td></tr></table></figure>
<ul>
<li>编译</li>
</ul>
<figure class="highlight elixir"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="variable">$ </span>make apps（这个过程可能由于网络等问题导致某些包无法下载，多执行几次就OK）</div><div class="line"><span class="variable">$ </span>make locales（编译多语言版本支持）</div></pre></td></tr></table></figure>
<p>注：若只编译简体中文版，需修改$HUE/desktop/core/src/desktop/settings.py文件中：LANGUAGE_CODE = ‘zh_CN’，并删除LANGUAGES中的其它语言项。</p>
<ul>
<li>安装</li>
</ul>
<figure class="highlight elixir"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="variable">$ </span>make install (默认安装到/usr/local目录下)</div><div class="line"><span class="variable">$ </span>make install PREFIX=<span class="regexp">/usr/hdp</span><span class="regexp">/2.3.4.0-3485/hue</span> (指定安装目录)</div></pre></td></tr></table></figure>
<ul>
<li>测试</li>
</ul>
<figure class="highlight awk"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ HUE<span class="regexp">/build/</span>env<span class="regexp">/bin/</span>hue test all</div></pre></td></tr></table></figure>
<h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h3><ul>
<li>添加用户,目录赋权</li>
</ul>
<figure class="highlight processing"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ adduser <span class="built_in">hue</span> (确保<span class="built_in">hue</span>用户与其home目录存在)</div><div class="line">$ chown -R <span class="built_in">hue</span>:<span class="built_in">hue</span> /usr/local/<span class="built_in">hue</span></div></pre></td></tr></table></figure>
<ul>
<li>启动Hue</li>
</ul>
<figure class="highlight awk"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ <span class="variable">$HUE</span><span class="regexp">/build/</span>env<span class="regexp">/bin/</span>supervisor &amp; (用户生产环境)</div><div class="line">$ <span class="variable">$HUE</span><span class="regexp">/build/</span>env<span class="regexp">/bin/</span>hue runcpserver &amp; (用户开发模式)</div></pre></td></tr></table></figure>
<p>Web浏览器：<a href="http://HUE_SERVER_HOST:8000" target="_blank" rel="external">http://HUE_SERVER_HOST:8000</a>  第一次输入的用户名密码为超级管理员的用户密码。</p>
<h2 id="问题剖析"><a href="#问题剖析" class="headerlink" title="问题剖析"></a>问题剖析</h2><ol>
<li><p>Hue 3.10.0以上版本编译过程中报错：error: ffi.h: No such file or directory。<br>问题分析：编译环境缺少libssl-devel libffi-devel,编译过程中若缺少一些依赖的工具包，会有很多类似的错误。<br>解决办法：yum -y install libssl-devel libffi-devel</p>
</li>
<li><p>Hue在启动过程中报错：UnicodeDecodeError: ‘ascii’ codec can’t decode byte 0xef in position 166: ordinal not in range(128)。<br>问题分析：编码问题，Python的str默认是ascii编码，和unicode编码冲突<br>解决办法：在出现问题的python文件或者页面添加如下3行信息：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line">reload(sys)</div><div class="line">sys.setdefaultencoding(<span class="string">'utf-8'</span>)</div></pre></td></tr></table></figure>
</li>
<li><p>Hue 3.9.0及以下版本自带LivyServer服务，支持Spark 1.6.0以下版本；<br>Hue 3.10.0及以上版本将不在集成LivyServer服务，支持Spark 1.6.0及以上版本，LivyServer服务需单独安装部署。</p>
</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hue是一个能够与Apache Hadoop交互的Web应用程序，一个开源的Apache Hadoop UI。&lt;br&gt;本文介绍Hue的编译、安装、启动过程以及记录这个过程中可能遇到的问题。&lt;/p&gt;
&lt;img src=&quot;/uploads/Hue-编译安装/hue-search.png&quot;&gt;
    
    </summary>
    
      <category term="BigData" scheme="https://eshareditor.github.io/categories/BigData/"/>
    
    
      <category term="Hue" scheme="https://eshareditor.github.io/tags/Hue/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://eshareditor.github.io/2016/09/12/hello-world/"/>
    <id>https://eshareditor.github.io/2016/09/12/hello-world/</id>
    <published>2016-09-12T03:04:07.189Z</published>
    <updated>2016-09-07T09:32:55.617Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>About Me</title>
    <link href="https://eshareditor.github.io/2016/09/09/About-Me/"/>
    <id>https://eshareditor.github.io/2016/09/09/About-Me/</id>
    <published>2016-09-09T11:00:00.000Z</published>
    <updated>2016-09-15T09:35:03.457Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>You can either travel or read,but either your body or soul must be on the way. </p>
<footer><strong>《Roman Holiday》</strong></footer></blockquote>
<img src="/uploads/About-Me/AboutMe_Travel.jpg">
<a id="more"></a>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;&lt;p&gt;You can either travel or read,but either your body or soul must be on the way. &lt;/p&gt;
&lt;footer&gt;&lt;strong&gt;《Roman Holiday》&lt;/strong&gt;&lt;/footer&gt;&lt;/blockquote&gt;
&lt;img src=&quot;/uploads/About-Me/AboutMe_Travel.jpg&quot;&gt;
    
    </summary>
    
      <category term="Life" scheme="https://eshareditor.github.io/categories/Life/"/>
    
    
      <category term="life" scheme="https://eshareditor.github.io/tags/life/"/>
    
  </entry>
  
</feed>
