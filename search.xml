<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
    
    <entry>
      <title><![CDATA[Hue 组件使用]]></title>
      <url>https://eshareditor.github.io/2016/09/16/Hue-Component-Use/</url>
      <content type="html"><![CDATA[<p>Hue作为一个能与Hadoop集群进行交互的Web UI，大大降低了用户对某些大数据组件的操作使用成本。本文通过一些简单实例来介绍Hue中某些组件的使用，如Oozie任务调度，Hbase BulkLoad数据导入，Pig数据加载等</p>
<img src="/uploads/Hue-Component-Use/Hue-Component-Use.jpg">
<a id="more"></a>
<h2 id="Oozie任务调度"><a href="#Oozie任务调度" class="headerlink" title="Oozie任务调度"></a>Oozie任务调度</h2><h3 id="MapReduce任务"><a href="#MapReduce任务" class="headerlink" title="MapReduce任务"></a>MapReduce任务</h3><p>说明：本例使用Hadoop自带的jar包中的WordCount任务，操作用户:admin</p>
<p>1.上传hadoop自带的jar包到HDFS上(这里上传到/user/admin/MapReduceJob目录下)</p>
<p>2.使用Hue中Workflow编辑器，添加Mapreduce任务，并做如下配置，如图：</p>
<img src="/uploads/Hue-Component-Use/Hue-Oozie-WordCount.jpg">
<p>propersities具体配置信息：<br><figure class="highlight dust"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div></pre></td><td class="code"><pre><div class="line"><span class="xml"><span class="tag">&lt;<span class="name">workflow-app</span> <span class="attr">name</span>=<span class="string">"Job-MR-wordcount"</span> <span class="attr">xmlns</span>=<span class="string">"uri:oozie:workflow:0.5"</span>&gt;</span></span></div><div class="line">    <span class="tag">&lt;<span class="name">start</span> <span class="attr">to</span>=<span class="string">"mapreduce-627c"</span>/&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">kill</span> <span class="attr">name</span>=<span class="string">"Kill"</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">message</span>&gt;</span>操作失败，错误消息[$<span class="template-variable">&#123;wf:errorMessage(wf:lastErrorNode())&#125;</span><span class="xml">]<span class="tag">&lt;/<span class="name">message</span>&gt;</span></span></div><div class="line">    <span class="tag">&lt;/<span class="name">kill</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">action</span> <span class="attr">name</span>=<span class="string">"mapreduce-627c"</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">map-reduce</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">job-tracker</span>&gt;</span>$<span class="template-variable">&#123;jobTracker&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">job-tracker</span>&gt;</span></span></div><div class="line">            <span class="tag">&lt;<span class="name">name-node</span>&gt;</span>$<span class="template-variable">&#123;nameNode&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">name-node</span>&gt;</span></span></div><div class="line">            <span class="tag">&lt;<span class="name">prepare</span>&gt;</span></div><div class="line">                  <span class="tag">&lt;<span class="name">delete</span> <span class="attr">path</span>=<span class="string">"$</span></span><span class="template-variable">&#123;nameNode&#125;</span><span class="xml"><span class="tag"><span class="string">/user/admin/mapreduce/output-wordcount"</span>/&gt;</span></span></div><div class="line">            <span class="tag">&lt;/<span class="name">prepare</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.map.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.examples.WordCount$TokenizerMapper<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">                <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.reduce.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.examples.WordCount$IntSumReducer<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">                <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.combine.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.examples.WordCount$IntSumReducer<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">                <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.input.fileinputformat.inputdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/admin/MapReduceJob/input<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">                <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.output.fileoutputformat.outputdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/admin/MapReduceJob/output-wordcount<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">                <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.output.key.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.io.Text<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">                <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.output.value.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.io.IntWritable<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">                <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.maps<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                    <span class="tag">&lt;<span class="name">value</span>&gt;</span>6<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">                <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.reduces<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                    <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">                <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.cluster.mapmemory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1024<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">                <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.cluster.reducememory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1024<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">                <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.queuename<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                    <span class="tag">&lt;<span class="name">value</span>&gt;</span>queue1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">                <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.mapper.new-api<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">                <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.reducer.new-api<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">                <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">map-reduce</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">ok</span> <span class="attr">to</span>=<span class="string">"End"</span>/&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">error</span> <span class="attr">to</span>=<span class="string">"Kill"</span>/&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">action</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">end</span> <span class="attr">name</span>=<span class="string">"End"</span>/&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">workflow-app</span>&gt;</span></div></pre></td></tr></table></figure></p>
<p>3.在Coordinator和Bundles页面给任务做更多的设置，如设置任务执行时间，绑定多个任务等。点击保存并提交任务，在Oozie的仪表盘和Yarn任务管理页面查看任务状态。</p>
<h3 id="Spark任务"><a href="#Spark任务" class="headerlink" title="Spark任务"></a>Spark任务</h3><p>说明：本例测试使用Hue提供的SparkFileCopy任务，操作用户:admin</p>
<p>1.上传jar包到HDFS某个目录</p>
<p>2.使用Hue中Workflow编辑器，添加Spark任务，并做如下配置，如图：</p>
<img src="/uploads/Hue-Component-Use/Hue-Oozie-SparkFileCopy.jpg">
<img src="/uploads/Hue-Component-Use/Hue-Oozie-SparkFileCopy1.jpg">
<p>3.保存并提交任务，在Oozie的仪表盘和Yarn任务管理页面查看任务状态。</p>
<h2 id="Hbase数据导入"><a href="#Hbase数据导入" class="headerlink" title="Hbase数据导入"></a>Hbase数据导入</h2><h3 id="BulkLoad数据导入"><a href="#BulkLoad数据导入" class="headerlink" title="BulkLoad数据导入"></a>BulkLoad数据导入</h3><p>1.在Hue的Hbase模块新建表：userinfo，两个列族：info和grade</p>
<p>2.在Windows本地某一目录新建数据文件：userinfo.csv，文件内容如下图</p>
<img src="/uploads/Hue-Component-Use/Hue-Hbase-BulkLoad-Userinfo.jpg">
<p>说明：<br>1)导入的文件中每行数据数量必须匹配对应的行数，数据可以不存在，但必须用对应的分隔符留出该字段位置。<br>2)因bulkload不支持多个分隔符划分字段，数据文件第一行每个列之间注意用单个分隔符分割，且不要留空格。</p>
<p>3.在Hue的Hbase模块打开userinfo表，点击BulkLoad加载本地数据文件userinfo.csv，数据导入成功</p>
<h2 id="Pig数据加载"><a href="#Pig数据加载" class="headerlink" title="Pig数据加载"></a>Pig数据加载</h2><p>Hue提供的Pig应用允许用户定义pig脚本、运行脚本、以及查看Job的工作状态。执行的Pig脚本会自动由Oozie提交到Yarn任务管理器上，所以在Oozie仪表盘以及Yarn任务管理都能看到此Pig任务的运行状态。可以看出在Hue中Pig的使用依赖Oozie。</p>
<p>1.在HDFS的某目录下(这里使用/user/admin/PigTestData目录)创建数据文件student.txt</p>
<p>student.txt数据文件内容：<br><figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">C01</span><span class="selector-pseudo">:N0101</span><span class="selector-pseudo">:82</span><span class="selector-pseudo">:90</span></div><div class="line"><span class="selector-tag">C01</span><span class="selector-pseudo">:N0102</span><span class="selector-pseudo">:59</span><span class="selector-pseudo">:68</span></div><div class="line"><span class="selector-tag">C01</span><span class="selector-pseudo">:N0103</span><span class="selector-pseudo">:65</span><span class="selector-pseudo">:73</span></div><div class="line"><span class="selector-tag">C02</span><span class="selector-pseudo">:N0201</span><span class="selector-pseudo">:81</span><span class="selector-pseudo">:88</span></div><div class="line"><span class="selector-tag">C02</span><span class="selector-pseudo">:N0202</span><span class="selector-pseudo">:94</span><span class="selector-pseudo">:99</span></div><div class="line"><span class="selector-tag">C02</span><span class="selector-pseudo">:N0203</span><span class="selector-pseudo">:79</span><span class="selector-pseudo">:92</span></div><div class="line"><span class="selector-tag">C03</span><span class="selector-pseudo">:N0301</span><span class="selector-pseudo">:56</span><span class="selector-pseudo">:67</span></div><div class="line"><span class="selector-tag">C03</span><span class="selector-pseudo">:N0302</span><span class="selector-pseudo">:92</span><span class="selector-pseudo">:84</span></div><div class="line"><span class="selector-tag">C03</span><span class="selector-pseudo">:N0306</span><span class="selector-pseudo">:72</span><span class="selector-pseudo">:49</span></div></pre></td></tr></table></figure></p>
<p>Pig脚本语句：<br><figure class="highlight cs"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">records = load <span class="string">'/user/admin/PigTestData/student.txt'</span> <span class="function"><span class="keyword">using</span> <span class="title">PigStorage</span>(<span class="params"><span class="string">','</span></span>) <span class="title">as</span>(<span class="params">classNo:chararray, studNo:chararray, score_Chinese:<span class="keyword">int</span>, score_Math:<span class="keyword">int</span></span>)</span>;</div><div class="line">dump records;</div><div class="line">store records <span class="keyword">into</span> <span class="string">'/user/admin/PigTestData/student_out'</span> <span class="function"><span class="keyword">using</span> <span class="title">PigStorage</span>(<span class="params"><span class="string">':'</span></span>)</span>;</div></pre></td></tr></table></figure></p>
<p>2.在Hue的Pig模块执行如下Pig脚本,内容如下图</p>
<img src="/uploads/Hue-Component-Use/Hue-Pig-LoadData.jpg">
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Hbase BulkLoad]]></title>
      <url>https://eshareditor.github.io/2016/09/14/Hbase-BulkLoad/</url>
      <content type="html"><![CDATA[<p>Hbase快速数据导入工具BulkLoad：利用Hbase的数据以特定的格式存储在HDFS内这一原理，通过MapReduce作业直接生成这种数据格式文件，然后上传到HDFS上合适的位置，完成大量数据快速入库。</p>
<img src="/uploads/Hbase-BulkLoad/Hbase-BulkLoad.jpg">
<a id="more"></a>
<h2 id="Hbase数据带入方式"><a href="#Hbase数据带入方式" class="headerlink" title="Hbase数据带入方式"></a>Hbase数据带入方式</h2><ul>
<li>HBase Client 调用方式</li>
<li>MapReduce 任务方式</li>
<li>BulkLoad 工具方式</li>
<li>Sqoop 工具方式</li>
</ul>
<h2 id="BulkLoad优劣势"><a href="#BulkLoad优劣势" class="headerlink" title="BulkLoad优劣势"></a>BulkLoad优劣势</h2><h3 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h3><ul>
<li>如果一次性入导入Hbase表的数据量巨大，不仅处理速度慢不说，还特别占用Region资源， 一个比较高效便捷的方法就是使用 “Bulk Loading”方法，即HBase提供的HFileOutputFormat类。</li>
<li>利用Hbase的数据以特定的格式存储在HDFS内这一原理，直接生成这种HDFS中存储的数据格式文件，然后上传至合适位置，即完成巨量数据快速入库的办法。配合mapreduce完成，高效便捷，而且不占用region资源，增添负载。</li>
</ul>
<h3 id="劣势"><a href="#劣势" class="headerlink" title="劣势"></a>劣势</h3><ul>
<li>仅适合初次数据导入，即表内数据为空。</li>
<li>HBase集群与Hadoop集群为同一集群，即HBase所基于的HDFS为生成HFile的MR的集群。</li>
</ul>
<h2 id="BulkLoad使用"><a href="#BulkLoad使用" class="headerlink" title="BulkLoad使用"></a>BulkLoad使用</h2><ol>
<li>从数据源（通常为文本文件或其它数据库）提取数据并上传到HDFS。</li>
<li>利用一个MapReduce作业准备数据：通过编排一个MapReduce作业，需手动编写map函数，作业需要使用rowkey(行键)作为输出Key，KeyValue、Put或者Delete作为输出Value。MapReduce作业需要使用Hbase提供的HFileOutputFormat2来生成Hbase底层存储的HFile数据格式文件。为了有效的导入数据，需要配置HFileOutputFormat2使得每一个输出文件都在一个合适的区域中。</li>
<li>告诉RegionServers数据的位置并导入数据，这一步需要使用Hbase中的LoadIncrementalHFiles，将文件在HDFS上的位置传递给它，它就会利用RegionServer将数据导入到相应的区域，完成与Hbase表的关联。<img src="/uploads/Hbase-BulkLoad/Hbase-BulkLoad.jpg" title="操作流程图">
</li>
</ol>
<h3 id="命令行导入"><a href="#命令行导入" class="headerlink" title="命令行导入"></a>命令行导入</h3><h4 id="直接导入数据"><a href="#直接导入数据" class="headerlink" title="直接导入数据"></a>直接导入数据</h4><p>在Hbase中创建空表，如userinfo,使用Hbase自带的ImportTsv工具导入/opt/userinfo.tsv数据文件</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hadoop jar $HBASE_HOME<span class="regexp">/lib/</span>hbase-server<span class="number">-1.1</span><span class="number">.2</span>.jar importtsv -Dimporttsv.columns=HBASE_ROW_KEY,<span class="string">cf1:</span>name,<span class="string">cf1:</span>sex,<span class="string">cf2:</span><span class="class"><span class="keyword">class</span> <span class="title">userinfo</span> /<span class="title">opt</span>/<span class="title">userinfo</span>.<span class="title">tsv</span></span></div></pre></td></tr></table></figure>
<h4 id="分步导入数据"><a href="#分步导入数据" class="headerlink" title="分步导入数据"></a>分步导入数据</h4><ul>
<li>将userinfo.tsv文件上传到HDFS某个目录，并在Hbase中创建userinfo表</li>
</ul>
<figure class="highlight awk"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hadoop fs -put <span class="regexp">/opt/u</span>serinfo.tsv <span class="regexp">/data_dir/</span>source_file</div></pre></td></tr></table></figure>
<ul>
<li>利用ImportTsv工具的Dimporttsv.bulk.output参数项指定生成的HFile文件位置/date_dir/bulkload_data/output</li>
</ul>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hadoop jar $HBASE_HOME/lib/hbase-server-<span class="number">1.1</span>.<span class="number">2</span>.jar importtsv -Dimporttsv.bulk.output=<span class="regexp">/date_dir/bulkload</span>_data/output -Dimporttsv.columns=HBASE_ROW_KEY,<span class="symbol">cf1:</span>name,<span class="symbol">cf1:</span>sex,<span class="symbol">cf2:</span><span class="class"><span class="keyword">class</span> <span class="title">userinfo</span> /<span class="title">opt</span>/<span class="title">userinfo</span>.<span class="title">tsv</span></span></div></pre></td></tr></table></figure>
<ul>
<li>利用completebulkload完成bluk load数据导入</li>
</ul>
<figure class="highlight crystal"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hadoop jar $HBASE_HOME/<span class="class"><span class="keyword">lib</span>/<span class="title">hbase</span>-<span class="title">server</span>-1.1.2.<span class="title">jar</span> <span class="title">completebulkload</span> /<span class="title">date_dir</span>/<span class="title">bulkload_data</span>/<span class="title">output</span> <span class="title">userinfo</span></span></div></pre></td></tr></table></figure>
<h3 id="相关说明"><a href="#相关说明" class="headerlink" title="相关说明"></a>相关说明</h3><ol>
<li>将数据文件生成HFile方式是所有的加载方案里面是最快的，前提是：Hbase表须为空！如果表中已经有数据，HFile再次导入的时候，HBase的表会触发split分割操作</li>
<li>最终输出结果，无论是Map还是Reduce，输出建议只使用<immutablebyteswritable, keyvalue=""></immutablebyteswritable,></li>
<li>指定的列和源文件字段必须全部匹配，不匹配则一定导入失败</li>
<li>命令行导入时默认的分隔符为tab符</li>
<li>RowKey可以在任何位置：<br>假设存在数据4列（逗号为分隔符）<br>1，2，3，4<br>第一列RowKey<br>RowKey，2，3，4<br>第三列RowKey<br>1，2，RowKey，4</li>
<li>常用参数说明：<br>-Dimporttsv.skip.bad.lines=false –&gt; 若遇到无效行则失败<br>‘-Dimporttsv.separator=|’或‘-Dimporttsv.separator=,’ –&gt; 指定分割符<br>-Dimporttsv.timestamp=currentTimeAsLong –&gt; 导入时使用指定的时间戳<br>-Dimporttsv.mapper.class=my.Mapper –&gt; 使用用户指定的Mapper类(默认的org.apache.hadoop.hbase.mapreduce.TsvImporterMapper)</li>
</ol>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p><a href="http://www.ibm.com/developerworks/cn/opensource/os-cn-data-import/index.html" target="_blank" rel="external">http://www.ibm.com/developerworks/cn/opensource/os-cn-data-import/index.html</a><br><a href="http://www.aboutyun.com/thread-11652-1-1.html" target="_blank" rel="external">http://www.aboutyun.com/thread-11652-1-1.html</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Hue 编译安装]]></title>
      <url>https://eshareditor.github.io/2016/09/14/Hue-%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85/</url>
      <content type="html"><![CDATA[<p>Hue是一个能够与Apache Hadoop交互的Web应用程序，一个开源的Apache Hadoop UI。<br>本文介绍Hue的编译、安装、启动过程以及记录这个过程中可能遇到的问题。</p>
<img src="/uploads/Hue-编译安装/hue-search.png">
<a id="more"></a>
<h2 id="编译、安装、启动"><a href="#编译、安装、启动" class="headerlink" title="编译、安装、启动"></a>编译、安装、启动</h2><h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><ul>
<li>IOS: Centos6.7  (minimum)</li>
<li>Maven 3.3.3</li>
<li>Ant 1.9.4+ </li>
<li>Python 2.6.6+</li>
</ul>
<h3 id="依赖的工具包"><a href="#依赖的工具包" class="headerlink" title="依赖的工具包"></a>依赖的工具包</h3><ul>
<li><p>Hue官网列出的工具包</p>
<blockquote>
<p>asciidoc krb5-devel libxml2-devel libxslt-devel libtidy mysql mysql-devel openldap-devel python-devel sqlite-devel make openssl-devel gcc gcc-c++ gmp-devel cyrus-sasl-plain cyrus-sasl-devel cyrus-sasl-gssapi libffi-devel</p>
</blockquote>
</li>
<li><p>额外需要的工具包(在编译过程中根据报错信息需额外安装的工具包)</p>
<blockquote>
<p>python-simplejson python-setuptools rsync saslwrapper-devel pycrypto libyaml-devel libsasl2-dev libsasl2-modules-gssapi-mit libkrb5-dev libssl-devel</p>
</blockquote>
</li>
<li><p>注意：Centos7系统安装：gmp libtidy mysql-devel工具无法安装。且在编译过程中提示需安装ipdb: centos7系统自带的python2.7，而默认情况下python2.7不提供pip工具的，需手动安装好，或者使用hue里的python2.7的pip工具来安装ipdb。命令：pip install ipdb</p>
</li>
</ul>
<h3 id="编译安装"><a href="#编译安装" class="headerlink" title="编译安装"></a>编译安装</h3><ul>
<li>下载</li>
</ul>
<figure class="highlight crmsh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ git <span class="keyword">clone</span> <span class="title">https</span>://github.com/cloudera/hue.git</div></pre></td></tr></table></figure>
<ul>
<li>编译</li>
</ul>
<figure class="highlight elixir"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="variable">$ </span>make apps（这个过程可能由于网络等问题导致某些包无法下载，多执行几次就OK）</div><div class="line"><span class="variable">$ </span>make locales（编译多语言版本支持）</div></pre></td></tr></table></figure>
<p>注：若只编译简体中文版，需修改$HUE/desktop/core/src/desktop/settings.py文件中：LANGUAGE_CODE = ‘zh_CN’，并删除LANGUAGES中的其它语言项。</p>
<ul>
<li>安装</li>
</ul>
<figure class="highlight elixir"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="variable">$ </span>make install (默认安装到/usr/local目录下)</div><div class="line"><span class="variable">$ </span>make install PREFIX=<span class="regexp">/usr/hdp</span><span class="regexp">/2.3.4.0-3485/hue</span> (指定安装目录)</div></pre></td></tr></table></figure>
<ul>
<li>测试</li>
</ul>
<figure class="highlight awk"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ HUE<span class="regexp">/build/</span>env<span class="regexp">/bin/</span>hue test all</div></pre></td></tr></table></figure>
<h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h3><ul>
<li>添加用户,目录赋权</li>
</ul>
<figure class="highlight processing"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ adduser <span class="built_in">hue</span> (确保<span class="built_in">hue</span>用户与其home目录存在)</div><div class="line">$ chown -R <span class="built_in">hue</span>:<span class="built_in">hue</span> /usr/local/<span class="built_in">hue</span></div></pre></td></tr></table></figure>
<ul>
<li>启动Hue</li>
</ul>
<figure class="highlight awk"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ <span class="variable">$HUE</span><span class="regexp">/build/</span>env<span class="regexp">/bin/</span>supervisor &amp; (用户生产环境)</div><div class="line">$ <span class="variable">$HUE</span><span class="regexp">/build/</span>env<span class="regexp">/bin/</span>hue runcpserver &amp; (用户开发模式)</div></pre></td></tr></table></figure>
<p>Web浏览器：<a href="http://HUE_SERVER_HOST:8000" target="_blank" rel="external">http://HUE_SERVER_HOST:8000</a>  第一次输入的用户名密码为超级管理员的用户密码。</p>
<h2 id="问题剖析"><a href="#问题剖析" class="headerlink" title="问题剖析"></a>问题剖析</h2><ol>
<li><p>Hue 3.10.0以上版本编译过程中报错：error: ffi.h: No such file or directory。<br>问题分析：编译环境缺少libssl-devel libffi-devel,编译过程中若缺少一些依赖的工具包，会有很多类似的错误。<br>解决办法：yum -y install libssl-devel libffi-devel</p>
</li>
<li><p>Hue在启动过程中报错：UnicodeDecodeError: ‘ascii’ codec can’t decode byte 0xef in position 166: ordinal not in range(128)。<br>问题分析：编码问题，Python的str默认是ascii编码，和unicode编码冲突<br>解决办法：在出现问题的python文件或者页面添加如下3行信息：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line">reload(sys)</div><div class="line">sys.setdefaultencoding(<span class="string">'utf-8'</span>)</div></pre></td></tr></table></figure>
</li>
<li><p>Hue 3.9.0及以下版本自带LivyServer服务，支持Spark 1.6.0以下版本；<br>Hue 3.10.0及以上版本将不在集成LivyServer服务，支持Spark 1.6.0及以上版本，LivyServer服务需单独安装部署。</p>
</li>
</ol>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Hexo 简介]]></title>
      <url>https://eshareditor.github.io/2016/09/12/hello-world/</url>
      <content type="html"><![CDATA[<p>Hexo: A fast, simple &amp; powerful blog framework, powered by Node.js.</p>
<img src="/uploads/Hello-World/Hello-World-Hexo.jpg">
<a id="more"></a>
<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[About Me]]></title>
      <url>https://eshareditor.github.io/2016/09/09/About-Me/</url>
      <content type="html"><![CDATA[<blockquote><p>You can either travel or read,but either your body or soul must be on the way. </p>
<footer><strong>《Roman Holiday》</strong></footer></blockquote>
<img src="/uploads/About-Me/AboutMe_Travel.jpg">
<a id="more"></a>
]]></content>
    </entry>
    
  
  
</search>
